{
  
    
        "post0": {
            "title": "The Image Classification",
            "content": "%matplotlib inline import torch import torchvision from torch.utils import data from torchvision import transforms import d2l . d2l.use_svg_display() . Reading the Dataset . ! pip install jupyterlab==3 -q . !pip install ipywidgets -q . ! jupyter nbextension enable --py widgetsnbextension . Enabling notebook extension jupyter-js-widgets/extension... - Validating: OK . !jupyter --version . jupyter core : 4.7.1 jupyter-notebook : 6.2.0 qtconsole : not installed ipython : 7.20.0 ipykernel : 5.4.3 jupyter client : 6.1.11 jupyter lab : 3.0.0 nbconvert : 6.0.7 ipywidgets : 7.6.5 nbformat : 5.1.2 traitlets : 5.0.5 . %config IPCompleter.greedy=True . # tensors. It divides all numbers by 255 so that all pixel values are between # 0 and 1 trans = transforms.ToTensor() . mnist_train = torchvision.datasets.FashionMNIST( root=&quot;../data&quot;,train=True, transform=trans, download=True) mnist_test = torchvision.datasets.FashionMNIST( root=&quot;../data&quot;,train=False, transform=trans, download=True) . The training dataset consists of 60000 images, the test set of 10000 images . len(mnist_train), len(mnist_test) . (60000, 10000) . Each training example is a 1 channel image 28 pixels by 28 pixels. We will refer to image size as heigh $h$ by width $w$: $h times w$ . mnist_train[0][0].shape . torch.Size([1, 28, 28]) . # @save def get_fashion_mnist_labels(labels): &quot;&quot;&quot; Return text labels for the Fashion-MNIST dataset&quot;&quot;&quot; text_labels = [&quot;t-shirt&quot;, &quot;trousers&quot;, &quot;pullover&quot;, &quot;dress&quot;, &quot;coat&quot;, &quot;sandal&quot;, &quot;shirt&quot;, &quot;sneaker&quot;, &quot;bag&quot;, &quot;ankle boot&quot;] return [text_labels[int(label)] for label in labels] . def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5): assert len(imgs) == num_rows*num_cols, &quot;number of images must match number of axes&quot; &quot;&quot;&quot;Plot a list of images&quot;&quot;&quot; # in pyplot figsize is width by rows figsize = (num_cols * scale, num_rows * scale) _, axes = d2l.plt.subplots(num_rows, num_cols, figsize = figsize) axes = axes.flatten() for i, (ax, img) in enumerate(zip(axes, imgs)): if torch.is_tensor(img): # Tensor Image ax.imshow(img.numpy()) else: # PIL Image ax.imshow(img) ax.axes.get_xaxis().set_visible(False) ax.axes.get_yaxis().set_visible(False) if titles: ax.set_title(titles[i]) return axes . Let&#39;s use the above function and plot the images with the corresponding labels . X, y = next(iter(data.DataLoader(mnist_train, batch_size=18))) show_images(X.reshape(18, 28, 28), num_rows=2, num_cols=9, titles=get_fashion_mnist_labels(y)); . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2022-01-16T10:08:31.201343 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/ To make our life easier when reading from the training and test sets, we use the built-in data iterator rather than creating one from scratch. Recall that at each iteration, a data iterator reads a minibatch of data with size batch_size each time. We also randomly shuffle the examples for the training data iterator. . batch_size = 256 # @save def get_dataloader_workers(): &quot;&quot;&quot;Use 4 processes to read the data.&quot;&quot;&quot; return 4 train_iter = data.DataLoader(mnist_train, batch_size, shuffle = True, num_workers = get_dataloader_workers()) . %%timeit for X,y in train_iter: continue . 2.12 s ± 20.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) . timer = d2l.Timer() for X,y in train_iter: continue f&#39;{timer.stop():.2f} sec&#39; . &#39;2.13 sec&#39; . Let&#39;s put it all together . def load_data_fashion_mnist(batch_size, resize=None): &quot;&quot;&quot;Download the Fashion-MNIST dataset and then load it into memory. Returns two DataLoaders - train and test &quot;&quot;&quot; # Get transformations trans = [transforms.ToTensor()] if resize: trans.insert(0, transforms.Resize(resize)) trans = transforms.Compose(trans) # Load train and test data mnist_train = torchvision.datasets.FashionMNIST( root=&quot;../data&quot;, train=True, download=True, transform=trans) mnist_test = torchvision.datasets.FashionMNIST( root=&quot;../data&quot;, train=False, download=True, transform=trans) # Return DataLoaders return (data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=get_dataloader_workers()), data.DataLoader(mnist_test, batch_size=batch_size, shuffle=True, num_workers=get_dataloader_workers())) . train_iter, test_iter = load_data_fashion_mnist(32, resize = 64) for X, y in train_iter: print(X.shape, X.dtype, y.shape, y.dtype) break . torch.Size([32, 1, 64, 64]) torch.float32 torch.Size([32]) torch.int64 . show_images(X.reshape(32,64,64), num_rows=4, num_cols=8, titles=get_fashion_mnist_labels(y)); . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2022-01-16T10:14:24.495312 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/ Implementing from scratch . %load_ext autoreload %autoreload 2 import torch from IPython import display import d2l . batch_size = 256 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size) . Each image is represented by 28x28 pixels. We will flatten them to get 784 array and will traet each pixel as an input. We also have 10 classes, so our $w$ is going to be $784 times 10$ matrix, and bias $b$ is $1 times 10$ row vector . num_inputs = 784 num_outputs = 10 W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True) b = torch.zeros(num_outputs, requires_grad=True) . X = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]) . X.sum(0, keepdim=True), X.sum(1, keepdim=True) . (tensor([[5., 7., 9.]]), tensor([[ 6.], [15.]])) . def softmax(X): X_exp = X.exp() X_softmax = X_exp.sum(1, keepdim=True) return X_exp / X_softmax . X = torch.normal(0, 1, (2, 5)) X_prob = softmax(X) X_prob, X_prob.sum(1, keepdim=True) . (tensor([[0.0695, 0.2975, 0.4530, 0.1100, 0.0701], [0.3566, 0.2505, 0.2452, 0.1181, 0.0296]]), tensor([[1.0000], [1.0000]])) . def net(X): &quot;&quot;&quot;Reshape as X can have additional dimension with value 1 (like image (1,28,28))&quot;&quot;&quot; return softmax(X.reshape((-1,W.shape[0]))@W + b) . y = torch.tensor([0, 2]) y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]]) y_hat[[0, 1], y] . tensor([0.1000, 0.5000]) . np.exp(-100000000000) . 0.0 . # break net(X), net(X).shape softmax(net(X)).sum(axis=1, keepdim=True) . torch.Size([256, 10]) . def cross_entropy(y_hat, y): &quot;&quot;&quot;From the above example we see that we need to take the logarithm only of the probability that corresponds to the actual class. Other probabilities will be multiplied by zero&quot;&quot;&quot; return -torch.log(y_hat[range(len(y_hat)), y]) cross_entropy(y_hat, y) . tensor([2.3026, 0.6931]) . def accuracy(y_hat, y): &quot;&quot;&quot;Compute the number of correct predictions.&quot;&quot;&quot; if len(y_hat.shape) &gt; 1 and y_hat.shape[1] &gt; 1: y_hat = y_hat.argmax(axis=1) cmp = y_hat.type(y.dtype) == y return float(cmp.sum()) . def evaluate_accuracy(net, data_iter): &quot;&quot;&quot;Compute the accuracy for a model on a dataset.&quot;&quot;&quot; if isinstance(net, torch.nn.Module): # set the model in evaluation mode net.eval() # Create two variables in the Accumulator to store number of correct predictions, # and number of predictions metric = Accumulator(2) with torch.no_grad(): for X, y in data_iter: metric.add(accuracy(net(X),y), y.numel()) return metric[0] / metric[1] . class Accumulator: &quot;&quot;&quot;For accumulating sums over `n` variables. used to store a list, where each elemetn represents some characteristic that is accumulated over iterable &quot;&quot;&quot; def __init__(self, n): self.data = [0.0] * n def add(self, *args): self.data = [a + float(b) for a,b in zip(self.data, args)] def reset(self): self.data = [0.0] * len(self.data) # implement __getitem__ to allow indexing (make subscriptable) def __getitem__(self, idx): return self.data[idx] . evaluate_accuracy(net, test_iter) . 0.117 . Training . def train_epoch_ch3(net, train_iter, loss, updater): &quot;&quot;&quot;The training loop defined in Chapter 3.&quot;&quot;&quot; # Set the model to training mode if isinstance(net, torch.nn.Module): net.train() # Store training loss, training accuracy and # training examples metric = Accumulator(3) for X, y in train_iter: # Calculate predictions y_hat = net(X) # Calculate loss l = loss(y_hat, y) if isinstance(updater, torch.optim.Optimizer): # Using Pytroch in-built optimizer and loss criterion updater.zero_grad() l.mean().backward() updater.step() else: # Using custom built optimizer and loss criterion. We use here `sum`, because # updater divides by the batch_size l.sum().backward() # updater calls zero_grad inside the function. X.shape[0] is the batch_size updater(X.shape[0]) #??? metric.add(float(l.sum()), accuracy(y_hat, y), y.numel()) # Return loss and training accuracy return metric[0] / metric[2], metric[1] / metric[2] . class Animator: &quot;&quot;&quot; For plotting data in animation.&quot;&quot;&quot; def __init__(self, xlabel=None, ylabel=None, legend=None,xlim=None, ylim=None, xscale=&#39;linear&#39;, yscale=&#39;linear&#39;, fmts=(&#39;-&#39;,&#39;m--&#39;,&#39;g-.&#39;,&#39;r:&#39;), nrows=1, ncols=1, figsize=(3.5, 2.5)): # Incrementally plot multiple lines if legend is None: legend = [] d2l.use_svg_display() self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize) if nrows * ncols == 1: pass #self.axes = [self.axes, ] #?? # Use a lambda function to capture arguments self.config_axes = lambda: d2l.set_axes( self.axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend) self.X, self.Y, self.fmts = None, None, fmts def add(self, x, y): &quot;&quot;&quot; Add multiple data points into the figure. We can think of `x` as epoch and `y` as metrics (train-accuracy, test-accuracy, loss, etc) &quot;&quot;&quot; if not hasattr(y, &quot;__len__&quot;): y = [y] n = len(y) if not hasattr(x, &quot;__len__&quot;): x = [x] * n # First iteration to allocate space for X and Y if not self.X: self.X = [[] for _ in range(n)] if not self.Y: self.Y = [[] for _ in range(n)] for i, (a, b) in enumerate(zip(x, y)): self.X[i].append(a) self.Y[i].append(b) self.axes.cla() for x, y, fmt in zip(self.X, self.Y, self.fmts): self.axes.plot(x, y, fmt) self.config_axes() display.display(self.fig) display.clear_output(wait=True) . The following training function then trains a model net on a training dataset accessed via train_iter for multiple epochs, which is specified by num_epochs. At the end of each epoch, the model is evaluated on a testing dataset accessed via test_iter. We will leverage the Animator class to visualize the training progress. . def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater): &quot;&quot;&quot;Train a model (defined in Chapter 3).&quot;&quot;&quot; animator = Animator(xlabel=&#39;epoch&#39;, xlim=[1, num_epochs], ylim=[0.3, 0.9], legend=[&#39;train loss&#39;, &#39;train acc&#39;, &#39;test_acc&#39;]) for epoch in range(num_epochs): train_metrics = train_epoch_ch3(net, train_iter, loss, updater) test_acc = evaluate_accuracy(net, test_iter) # We concatinate tuples, that&#39;s why we need to add &#39;, )&#39; animator.add(epoch+1, train_metrics + (test_acc, )) train_loss, train_acc = train_metrics assert train_loss &lt; 0.5, train_loss assert train_acc &lt;= 1 and train_acc &gt; 0.7, train_acc assert test_acc &lt;= 1 and test_acc &gt; 0.7, test_acc . lr = 0.1 def updater(batch_size): return d2l.sgd([W,b], lr, batch_size) . num_epochs = 10 train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2022-01-21T12:48:36.386782 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/ y_ = (1,2) + (3,) x_ = [1]*len(y_) X_ = [[] for _ in range(len(y_))] Y_ = [[] for _ in range(len(y_))] for i, (a, b) in enumerate(zip(x_, y_)): X_[i] = a Y_[i] = b . Concise Implementation . %load_ext autoreload %autoreload 2 import torch from torch import nn import d2l . The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload . #pip install jedi==0.17.2 . batch_size = 256 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size) . # layer to reshape the inputs before the linear layer in our network net = nn.Sequential(nn.Flatten(), nn.Linear(28*28, 10)) def init_weights(m): if type(m) == nn.Linear: nn.init.normal_(m.weight, std=0.01) net.apply(init_weights); . loss = nn.CrossEntropyLoss(reduction=&#39;none&#39;) . trainer = torch.optim.SGD(net.parameters(), lr=0.1) . num_epochs=10 d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2022-01-21T14:09:27.576149 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/ loss . CrossEntropyLoss() .",
            "url": "https://iamalos.github.io/deep-learning/2022/01/21/SoftmaxRegression.html",
            "relUrl": "/2022/01/21/SoftmaxRegression.html",
            "date": " • Jan 21, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Title",
            "content": "Linear Regression from Scratch . Import libraries . %matplotlib inline %load_ext autoreload %autoreload 2 import random import torch import d2l . In the following code snippet, we generate a dataset containing 1 000 examples, each consisting of 2 features sampled from a standard normal distribution. Thus our synthetic dataset will be a matrix $ X in mathbb{R} ^{1000x2} $ . The true parameters generating the dataset will be $ textbf{w} = [2, -3.4]^T$ and $b = 4.2$. Synthetic labels will follow the following linear model with error term $ epsilon $: $$ textbf{y} = textbf{X} textbf{w} +b + epsilon $$ . def synthetic_data(w, b, num_examples): &quot;&quot;&quot;Generate y = Xw + b + noise&quot;&quot;&quot; X = torch.normal(0, 1, (num_examples,len(w))) y = X @ w + b y += torch.normal(0, 0.01, y.shape) return X, y.reshape((-1,1)) . true_w = torch.tensor([2, -3.4]) true_b = 4.2 features, labels = synthetic_data( true_w, true_b, 10000) . print(&#39;features:&#39;, features[0], &#39; nlabel:&#39;, labels[0]) . features: tensor([-0.8342, -0.8385]) label: tensor([5.3736]) . d2l.set_figsize() d2l.plt.scatter(features[:,1], labels, 1); . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2022-01-08T19:05:38.044908 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/ Next we define a function to take a minibatch of data of size batch_size . def data_iter(batch_size, features, labels): num_examples = len(features) indices = list(range(num_examples)) random.shuffle(indices) for i in range(0, num_examples, batch_size): batch_indices = torch.tensor( indices[i: min(i + batch_size, num_examples)]) yield features[batch_indices], labels[batch_indices] . batch_size = 10 for X,y in data_iter(batch_size, features, labels): print(X, &#39; n&#39;, y) break . Initialize the weights . w = torch.normal(0, 1, (2,1), requires_grad=True) b = torch.zeros(1, requires_grad=True) . def linreg(X, w, b): &quot;&quot;&quot;The linear regression model&quot;&quot;&quot; return X @ w + b . def squared_loss(y_hat, y): return 0.5 * (y_hat-y.reshape(y_hat.shape))**2 . def sgd(params, lr, batch_size): with torch.no_grad(): for param in params: param += -lr * param.grad / batch_size param.grad.zero_() . . Inititalize parameters ($ mathbf{w}, b$) | Repeat until done Compute gradient $ textbf{g} leftarrow partial_{ mathbf{w}, b} frac{1}{|B|} sum_{i in B} l( mathbf{x}^{(i)}, y^{(i)}, mathbf{w}, b) $ | Update parameters $ ( mathbf{w}, b) leftarrow ( mathbf{w}, b) - eta mathbf{g} $ | . | . lr = 0.03 num_epochs = 3 net = linreg loss = squared_loss for epoch in range(num_epochs): for X, y in data_iter(batch_size, features, labels): # Minibatch loss in `X` and `y` l = loss(net(X, w, b), y) # Compute gradient on `l` with respect to [`w`, `b`] l.sum().backward() # Update parameters using their gradient sgd([w, b], lr, batch_size) with torch.no_grad(): train_l = loss(net(features, w, b), labels) print(f&#39;epoch {epoch + 1}, loss {float(train_l.mean()):f}&#39;) . epoch 1, loss 0.000050 epoch 2, loss 0.000050 epoch 3, loss 0.000050 . print(f&#39;error in estimating w: {true_w - w.reshape(true_w.shape)}&#39;) print(f&#39;error in estimating b: {true_b - b}&#39;) . error in estimating w: tensor([4.0579e-04, 7.4625e-05], grad_fn=&lt;SubBackward0&gt;) error in estimating b: tensor([5.2452e-05], grad_fn=&lt;RsubBackward1&gt;) . Now leveraging PyTorch . true_w = torch.tensor([2, -3.4]) true_b = 4.2 features, labels = d2l.synthetic_data(true_w, true_b, 1000) . def load_array(data_arrays, batch_size, is_train=True): &quot;&quot;&quot;Construct a PyTorch data iterator&quot;&quot;&quot; dataset = data.TensorDataset(*data_arrays) return data.DataLoader(dataset, batch_size, shuffle=is_train) batch_size = 10 data_iter = load_array((features, labels), batch_size) . next(iter(data_iter)) . [tensor([[ 1.2632, -1.4318], [-1.2828, 0.5169], [ 0.6646, -0.7752], [-0.3978, -0.1792], [-0.4321, 1.3309], [ 0.5222, 0.9050], [-0.8075, 0.4358], [ 0.3331, -0.1475], [ 1.2248, 1.0001], [-1.1675, 0.0587]]), tensor([[11.5908], [-0.1005], [ 8.1534], [ 4.0223], [-1.1992], [ 2.1577], [ 1.1017], [ 5.3629], [ 3.2534], [ 1.6555]])] . from torch import nn # Define the model net = nn.Sequential(nn.Linear(2, 1)) # Initialize model parameters net[0].weight.data.normal_(0, 0.01) net[0].bias.data.fill_(0) # Define the loss function loss = nn.MSELoss() # Define the Optimization Algorithm trainer = torch.optim.SGD(net.parameters(), lr=0.03) . num_epochs = 3 loss_array = [] for epoch in range(num_epochs): for X,y in data_iter: l = loss(net(X), y) loss_array.append(l.detach().numpy()) trainer.zero_grad() l.backward() trainer.step() l = loss(net(features), labels) print(f&#39;epoch {epoch + 1}, loss {l:f}&#39;) . epoch 1, loss 0.000208 epoch 2, loss 0.000093 epoch 3, loss 0.000093 . x_for_plot = np.arange(len(loss_array)) y_for_plot = np.array(loss_array) d2l.plot(x_for_plot, y_for_plot, &#39;training loop&#39;, &#39;loss&#39;, legend=[&quot;MSE Loss&quot;], figsize=(6,5)) . Y has one axis . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2022-01-09T09:55:32.533048 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/ w = net[0].weight.data print(&#39;error in estimating w:&#39;, true_w - w.reshape(true_w.shape)) b = net[0].bias.data print(&#39;error in estimating b:&#39;, true_b - b) . error in estimating w: tensor([-7.2241e-05, 5.5385e-04]) error in estimating b: tensor([0.0003]) .",
            "url": "https://iamalos.github.io/deep-learning/2022/01/08/LinRegFromScretch.html",
            "relUrl": "/2022/01/08/LinRegFromScretch.html",
            "date": " • Jan 8, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Getting to speed with PyTorch",
            "content": "import torch . x = torch.arange(12, dtype=torch.float32) x,type(x) . (tensor([ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11.]), torch.Tensor) . x.shape, x.numel() . (torch.Size([12]), 12) . x.reshape(3,4) . tensor([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.]]) . torch.zeros(2,3,4) . tensor([[[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]], [[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]]]) . torch.ones(2,3,4) . tensor([[[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]], [[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]]]) . torch.randn(3,4) . tensor([[ 0.3692, -0.3183, -1.0832, -0.7684], [ 0.2825, -0.7007, -0.6273, 0.5732], [-0.2347, -0.3088, 1.0403, -1.1551]]) . torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]) . tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]) . Operations . x = torch.tensor([1.0, 2, 4, 8]) y = torch.tensor([2, 2, 2, 2]) x + y, x - y, x * y, x / y, x ** y # The ** operator is exponentiation . (tensor([ 3., 4., 6., 10.]), tensor([-1., 0., 2., 6.]), tensor([ 2., 4., 8., 16.]), tensor([0.5000, 1.0000, 2.0000, 4.0000]), tensor([ 1., 4., 16., 64.])) . torch.exp(x) . tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03]) . X = torch.arange(12, dtype=torch.float32).reshape((3,4)) Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]) torch.cat((X,Y),dim=0), torch.cat((X,Y),dim=1) . (tensor([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [ 2., 1., 4., 3.], [ 1., 2., 3., 4.], [ 4., 3., 2., 1.]]), tensor([[ 0., 1., 2., 3., 2., 1., 4., 3.], [ 4., 5., 6., 7., 1., 2., 3., 4.], [ 8., 9., 10., 11., 4., 3., 2., 1.]])) . X==Y . tensor([[False, True, False, True], [False, False, False, False], [False, False, False, False]]) . Broadcasting . a = torch.arange(3).reshape((3, 1)) b = torch.arange(2).reshape((1, 2)) a, b . (tensor([[0], [1], [2]]), tensor([[0, 1]])) . a+b . tensor([[0, 1], [1, 2], [2, 3]]) . torch.broadcast_tensors(a,b) . (tensor([[0, 0], [1, 1], [2, 2]]), tensor([[0, 1], [0, 1], [0, 1]])) . Saving Memory . before = id(Y) Y = Y + X id(Y) == before . False . This might be undesirable for two reasons. First, we do not want to run around allocating memory unnecessarily all the time. In machine learning, we might have hundreds of megabytes of parameters and update all of them multiple times per second. Typically, we will want to perform these updates in place. Second, we might point at the same parameters from multiple variables. If we do not update in place, other references will still point to the old memory location, making it possible for parts of our code to inadvertently reference stale parameters. . Z = torch.zeros_like(Y) print(&#39;id(Z):&#39;, id(Z)) Z[:] = X + Y print(&#39;id(Z):&#39;, id(Z)) . id(Z): 140116150347264 id(Z): 140116150347264 . before = id(X) X += Y id(X) == before . True .",
            "url": "https://iamalos.github.io/deep-learning/fastpages/jupyter/2022/01/01/Data_Manipulation.html",
            "relUrl": "/fastpages/jupyter/2022/01/01/Data_Manipulation.html",
            "date": " • Jan 1, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Title",
            "content": "import torch import pandas as pd import os . os.makedirs(os.path.join(&#39;..&#39;, &#39;data&#39;), exist_ok=True) data_file = os.path.join(&#39;..&#39;, &#39;data&#39;, &#39;house_tiny.csv&#39;) with open(data_file, &#39;w&#39;) as f: f.write(&#39;NumRooms,Alley,Price n&#39;) # Column names f.write(&#39;NA,Pave,127500 n&#39;) # Each row represents a data example f.write(&#39;2,NA,106000 n&#39;) f.write(&#39;4,NA,178100 n&#39;) f.write(&#39;NA,NA,140000 n&#39;) . cat ../data/house_tiny.csv . NumRooms,Alley,Price NA,Pave,127500 2,NA,106000 4,NA,178100 NA,NA,140000 . data = pd.read_csv(data_file) data . NumRooms Alley Price . 0 NaN | Pave | 127500 | . 1 2.0 | NaN | 106000 | . 2 4.0 | NaN | 178100 | . 3 NaN | NaN | 140000 | . inputs, outputs = data.iloc[:,0:2], data.iloc[:,-1] inputs = inputs.fillna(inputs.mean()) inputs . NumRooms Alley . 0 3.0 | Pave | . 1 2.0 | NaN | . 2 4.0 | NaN | . 3 3.0 | NaN | . inputs = pd.get_dummies(inputs, dummy_na=True) inputs . NumRooms Alley_Pave Alley_nan . 0 3.0 | 1 | 0 | . 1 2.0 | 0 | 1 | . 2 4.0 | 0 | 1 | . 3 3.0 | 0 | 1 | . X, y = torch.tensor(inputs.values), torch.tensor(outputs.values) X, y . (tensor([[3., 1., 0.], [2., 0., 1.], [4., 0., 1.], [3., 0., 1.]], dtype=torch.float64), tensor([127500, 106000, 178100, 140000])) . A = torch.arange(20, dtype=torch.float32).reshape(5, 4) A . tensor([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.], [16., 17., 18., 19.]]) . A.sum(axis=0), A.sum(axis=1) . (tensor([40., 45., 50., 55.]), tensor([ 6., 22., 38., 54., 70.])) . A.sum(axis=1, keepdims=True) . tensor([[ 6.], [22.], [38.], [54.], [70.]]) . A.cumsum(axis=0) . tensor([[ 0., 1., 2., 3.], [ 4., 6., 8., 10.], [12., 15., 18., 21.], [24., 28., 32., 36.], [40., 45., 50., 55.]]) . x = torch.arange(4, dtype=torch.float32) . A.shape, x.shape, torch.mv(A,x) . (torch.Size([5, 4]), torch.Size([4]), tensor([ 14., 38., 62., 86., 110.])) . B = torch.ones(4, 3) torch.mm(A, B) . tensor([[ 6., 6., 6.], [22., 22., 22.], [38., 38., 38.], [54., 54., 54.], [70., 70., 70.]]) . Calculus . %matplotlib inline import numpy as np from IPython import display import torch def f(x): return 3 * x**2 - 4*x . def numerical_lim(f, x, h): return (f(x + h) - f(x)) / h h = 0.1 for i in range(5): print(f&#39;h={h:.5f}, numerical limit={numerical_lim(f, 1, h):.5f}&#39;) h *= 0.1 . h=0.10000, numerical limit=2.30000 h=0.01000, numerical limit=2.03000 h=0.00100, numerical limit=2.00300 h=0.00010, numerical limit=2.00030 h=0.00001, numerical limit=2.00003 . print(f&#39;a={1+1:.5f}&#39;) . a=2.00000 . Plotting functions . def use_svg_display(): &quot;&quot;&quot;Use the svg format to display a plot in Jupyter.&quot;&quot;&quot; display.set_matplotlib_formats(&#39;svg&#39;) . def set_figsize(figsize=(3.5, 2.5)): &quot;&quot;&quot;Set the figure size for matplotlib&quot;&quot;&quot; use_svg_display() d2l.plt.rcParams[&#39;figure.figsize&#39;] = figsize . def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale,legend): &quot;&quot;&quot;Set the axes for matplotlib&quot;&quot;&quot; axes.set_xlabel(xlabel) axes.set_ylabel(ylabel) axes.set_xlim(xlim) axes.set_ylim(ylim) axes.set_xscale(xscale) axes.set_yscale(yscale) if legend: axes.legend(legend) axes.grid() . def plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None, ylim=None, xscale=&#39;linear&#39;, yscale=&#39;linear&#39;, fmts=(&#39;-&#39;, &#39;m--&#39;, &#39;g-.&#39;, &#39;r:&#39;), figsize=(3.5, 2.5), axes=None): &quot;&quot;&quot;Plot data points&quot;&quot;&quot; if legend is None: legend = [] set_figsize(figsize) axes = axes if axes else d2l.plt.gca() # Return True if &#39;X&#39; (tensor or list) has 1 axis def has_one_axis(X): return (hasattr(X, &quot;ndim&quot;) and X.ndim == 1 or isinstance(X, list) and not hasattr(X[0], &quot;__len__&quot;)) if has_one_axis(X): X = [X] print(&quot;X has one axis&quot;) if Y is None: # use X zero points as X and X as Y X, Y = [[]] * len(X), X elif has_one_axis(Y): Y = [Y] print(&quot;Y has one axis&quot;) # if different length, than take X for each Y output: &quot;&quot;&quot; x = [1,2,3] y = [f(x), f(x+a)] Then we will plot x,f(x) and x,f(x+a) &quot;&quot;&quot; if len(X) != len(Y): X = X * len(Y) print(&quot;Adjust length of X&quot;) # clear current axes axes.cla() for x, y, fmt in zip(X, Y, fmts): if len(x): axes.plot(x, y, fmt) else: axes.plot(y, fmt) set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend) . x = np.arange(0, 3, 0.1) plot(x, [f(x), 2*x-3], &#39;x&#39;, &#39;f(x)&#39;, legend=[&#39;f(x)&#39;, &#39;Tangent line(x=1)&#39;]) . X has one axis Adjust length of X . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2022-01-03T15:20:01.747190 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/ Exercises . import d2l import numpy as np %load_ext autoreload %autoreload 2 . X = np.arange(0.1, 10, 0.1) Y = X**3 - (1/X) Y_prime = 3*X**2 + (1/X)**2 . d2l.plot(X,[Y, Y_prime], xlabel=&#39;x&#39;, ylabel=&#39;f(x)&#39;, legend=[&#39;f(x)&#39;, &#39;Tangent line(x=1)&#39;]) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2022-01-04T16:18:24.485229 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/ Automatic Differentiation . import torch . Calculate gradient of $y=2X^TX$ . x = torch.arange(4.0) x . tensor([0., 1., 2., 3.]) . x.requires_grad_(True) x.grad # the default value is None . y = 2 * torch.dot(x, x) y, y.shape . (tensor(28., grad_fn=&lt;MulBackward0&gt;), torch.Size([])) . y.backward() x.grad . tensor([ 0., 4., 8., 12.]) . 4*x == x.grad . tensor([True, True, True, True]) . # values x.grad.zero_() y = x.sum() y.backward() x.grad . tensor([1., 1., 1., 1.]) . # which specifies the gradient of the differentiated function w.r.t `self`. # In our case, we simply want to sum the partial derivatives, so passing # in a gradient of ones is appropriate x.grad.zero_() y = x * x # y.backward(torch.ones(len(x))) equivalent to the below y.sum().backward() x.grad . tensor([0., 2., 4., 6.]) . x.grad.zero_() y = x * x u = y.detach() z = u * x z.backward(gradient=torch.ones(len(x))) #z.sum().backward() x.grad == u . tensor([True, True, True, True]) . def f(a): b = a * 2 while b.norm() &lt; 1000: b = b * 2 if b.sum() &gt; 0: c = b else: c = 100 * b return c . a = torch.rand(size=(), requires_grad=True) d = f(a) d.backward() . a.grad == d / a . tensor(True) . Exercise . x = torch.arange(-3, 3, 0.1, requires_grad = True) f = torch.sin(x) # we use sum because `backward()` expects to be called on a scalar! f.sum().backward() . d2l.plot(x.detach(), [f.detach(),x.grad.detach()], xlabel=&#39;x&#39;, ylabel=&#39;f(x0)&#39;, legend=[&#39;sin(x)&#39;, &#39;grad w.s.t x&#39;], ylim=[-1,1]) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2022-01-04T17:30:39.789977 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/ x.grad == torch.cos(x) . tensor([True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]) . f.sum() . tensor(-0.1411, grad_fn=&lt;SumBackward0&gt;) . print(dir(torch.distributions)) . [&#39;AbsTransform&#39;, &#39;AffineTransform&#39;, &#39;Bernoulli&#39;, &#39;Beta&#39;, &#39;Binomial&#39;, &#39;CatTransform&#39;, &#39;Categorical&#39;, &#39;Cauchy&#39;, &#39;Chi2&#39;, &#39;ComposeTransform&#39;, &#39;ContinuousBernoulli&#39;, &#39;CorrCholeskyTransform&#39;, &#39;Dirichlet&#39;, &#39;Distribution&#39;, &#39;ExpTransform&#39;, &#39;Exponential&#39;, &#39;ExponentialFamily&#39;, &#39;FisherSnedecor&#39;, &#39;Gamma&#39;, &#39;Geometric&#39;, &#39;Gumbel&#39;, &#39;HalfCauchy&#39;, &#39;HalfNormal&#39;, &#39;Independent&#39;, &#39;Kumaraswamy&#39;, &#39;LKJCholesky&#39;, &#39;Laplace&#39;, &#39;LogNormal&#39;, &#39;LogisticNormal&#39;, &#39;LowRankMultivariateNormal&#39;, &#39;LowerCholeskyTransform&#39;, &#39;MixtureSameFamily&#39;, &#39;Multinomial&#39;, &#39;MultivariateNormal&#39;, &#39;NegativeBinomial&#39;, &#39;Normal&#39;, &#39;OneHotCategorical&#39;, &#39;OneHotCategoricalStraightThrough&#39;, &#39;Pareto&#39;, &#39;Poisson&#39;, &#39;PowerTransform&#39;, &#39;RelaxedBernoulli&#39;, &#39;RelaxedOneHotCategorical&#39;, &#39;SigmoidTransform&#39;, &#39;SoftmaxTransform&#39;, &#39;StackTransform&#39;, &#39;StickBreakingTransform&#39;, &#39;StudentT&#39;, &#39;TanhTransform&#39;, &#39;Transform&#39;, &#39;TransformedDistribution&#39;, &#39;Uniform&#39;, &#39;VonMises&#39;, &#39;Weibull&#39;, &#39;__all__&#39;, &#39;__builtins__&#39;, &#39;__cached__&#39;, &#39;__doc__&#39;, &#39;__file__&#39;, &#39;__loader__&#39;, &#39;__name__&#39;, &#39;__package__&#39;, &#39;__path__&#39;, &#39;__spec__&#39;, &#39;bernoulli&#39;, &#39;beta&#39;, &#39;biject_to&#39;, &#39;binomial&#39;, &#39;categorical&#39;, &#39;cauchy&#39;, &#39;chi2&#39;, &#39;constraint_registry&#39;, &#39;constraints&#39;, &#39;continuous_bernoulli&#39;, &#39;dirichlet&#39;, &#39;distribution&#39;, &#39;exp_family&#39;, &#39;exponential&#39;, &#39;fishersnedecor&#39;, &#39;gamma&#39;, &#39;geometric&#39;, &#39;gumbel&#39;, &#39;half_cauchy&#39;, &#39;half_normal&#39;, &#39;identity_transform&#39;, &#39;independent&#39;, &#39;kl&#39;, &#39;kl_divergence&#39;, &#39;kumaraswamy&#39;, &#39;laplace&#39;, &#39;lkj_cholesky&#39;, &#39;log_normal&#39;, &#39;logistic_normal&#39;, &#39;lowrank_multivariate_normal&#39;, &#39;mixture_same_family&#39;, &#39;multinomial&#39;, &#39;multivariate_normal&#39;, &#39;negative_binomial&#39;, &#39;normal&#39;, &#39;one_hot_categorical&#39;, &#39;pareto&#39;, &#39;poisson&#39;, &#39;register_kl&#39;, &#39;relaxed_bernoulli&#39;, &#39;relaxed_categorical&#39;, &#39;studentT&#39;, &#39;transform_to&#39;, &#39;transformed_distribution&#39;, &#39;transforms&#39;, &#39;uniform&#39;, &#39;utils&#39;, &#39;von_mises&#39;, &#39;weibull&#39;] . help(d2l.plot) . Help on function plot in module d2l: plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None, ylim=None, xscale=&#39;linear&#39;, yscale=&#39;linear&#39;, fmts=(&#39;-&#39;, &#39;m--&#39;, &#39;g-.&#39;, &#39;r:&#39;), figsize=(3.5, 2.5), axes=None, verbose=False) Plot data points .",
            "url": "https://iamalos.github.io/deep-learning/2022/01/01/DataPreprocessing.html",
            "relUrl": "/2022/01/01/DataPreprocessing.html",
            "date": " • Jan 1, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://iamalos.github.io/deep-learning/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://iamalos.github.io/deep-learning/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://iamalos.github.io/deep-learning/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}